# CME 295 Lecture 6 详细笔记：Transformers & Large Language Models——推理模型与强化学习扩展

**授课人**：Afshine Amidi & Shervine Amidi（Stanford University）

**核心主题**：传统大语言模型的短板、推理模型的构建与评估、强化学习在推理能力提升中的应用（GRPO算法）、实际推理模型训练案例（DeepSeek R1）及模型蒸馏

## 一、课前回顾：大模型训练与RL基础

### 1.1 大模型核心训练阶段

模型从基础到落地的三阶段递进，是推理模型训练的基础：

- **预训练（Pretraining）**：初始化模型，让模型掌握语言、代码等基础认知，形成底座模型；

- **微调（Finetuning）**：针对具体任务调优，适配特定场景需求；

- **偏好调优（Preference tuning）**：对齐人类偏好，提升回答的贴合性与实用性。

### 1.2 LLM与强化学习（RL）的概念映射

大模型的token生成过程可完全对应RL的智能体框架，是RL提升LLM能力的核心逻辑：

|强化学习概念|大语言模型对应概念|具体解释|
|---|---|---|
|Agent（智能体）|LLM本身|模型作为决策与生成的主体|
|State（状态）|Input so far（当前输入上下文）|模型已接收的文本信息，决定后续生成方向|
|Reward（奖励）|Human preference（人类偏好）|对模型输出质量的评价指标|
|Action（动作）|Next token（下一个生成的token）|模型的每一步决策为生成单个token|
|Environment（环境）|Tokens（所有token集合）|模型生成token的取值空间|
|Policy（策略）|Probability of next token|模型基于上下文输出各token的概率分布|
### 1.3 PPO算法核心损失函数

PPO（近端策略优化）是提升LLM的经典RL算法，核心目标为**最大化优势值+限制与基础模型的偏差**，两种核心形式：

1. **PPO-clip**：通过裁剪策略比避免模型更新幅度过大

 $L^{CLIP}(\theta)=\hat{\mathbb{E}}_{t}\left[min \left(r_{t}(\theta) \hat{A}_{t}, clip\left(r_{t}(\theta), 1-\epsilon, 1+\epsilon\right) \hat{A}_{t}\right)\right]$ 

1. **PPO-KL惩罚**：通过KL散度软约束新策略与旧策略的差异

 $L^{KLPEN}(\theta)=\hat{\mathbb{E}}_{t}\left[\frac{\pi_{\theta}\left(a_{t} | s_{t}\right)}{\pi_{\theta_{old }}\left(a_{t} | s_{t}\right)} \hat{A}_{t}-\beta KL\left[\pi_{\theta_{old }}\left(\cdot | s_{t}\right), \pi_{\theta}\left(\cdot | s_{t}\right)\right]\right]$ 

- 注： $r_t(\theta)$ 为策略比， $\hat{A}_t$ 为优势值， $\beta$ 为KL散度惩罚系数。

## 二、传统大语言模型（Vanilla LLM）的优劣势

传统LLM的固有短板是**推理模型诞生的核心动因**，本讲聚焦推理能力的解决，其余短板为7、8讲重点：

### 2.1 核心优势

1. 擅长**模仿生成与创意构思**（如文本创作、内容生成）；

2. 代码生成与调试能力突出，是目前LLM最成熟的应用场景。

### 2.2 核心短板

1. **推理能力有限**：仅能做信息提取，无法完成复杂的逻辑、数学、代码推导；

2. **知识静态化**：预训练知识无法实时更新，对新信息缺乏理解；

3. **无法执行动作**：仅能生成文本，不能与外部工具/环境交互完成任务；

4. **难以评估**：模型输出的推理正确性缺乏高效量化评估方法。

## 三、推理模型的核心概念与技术基础

### 3.1 推理的定义与边界

- **暂定定义**：Reasoning = Ability to solve a problem（解决问题的能力）；

- **推理vs非推理**：核心区别为是否需要**逻辑推导/计算/步骤拆解**，而非纯信息检索：

|    非推理（Not reasoning）|    推理（Reasoning）|
|---|---|
|    斯坦福Transformer&LLM课程的课程代码是什么？|    小熊2020年出生，现在几岁？|
### 3.2 提升推理能力的核心策略：思维链（Chain of Thought, CoT）

2022年Wei等人提出，是所有推理模型的基础，**核心思想**：让模型在回答前先解释推理过程，而非直接输出答案。

#### 3.2.1 输出范式的变革

- 传统LLM：**问题 → LLM → 答案**（无中间过程，易出错）；

- 推理模型：**问题 → LLM → 推理链 → 答案**（输出=推理链+答案，可验证、可解释）。

#### 3.2.2 核心延伸思路

推理模型的进阶方向是**大规模CoT应用**：通过预训练、微调、RL让模型主动生成高质量推理链，而非仅小样本提示。

### 3.3 推理模型的发展趋势（2024-2025主流模型发布时间线）

推理模型为行业研发重点，以下为各大AI实验室首款推理模型的发布节点（非等比例，仅作演示）：

- 2024.09.12：DeepSeek R1

- 2024.12.19：OpenAI o1-preview

- 2025.01.20：Claude 3.7 Sonnet

- 2025.02.19：Gemini 2.0 Flash Thinking

- 2025.02.24：Magistral

- 2025.06.10：Grok 3 Beta

### 3.4 推理模型的识别特征与计费特点

#### 3.4.1 核心识别特征

以ChatGPT 5 Thinking为例，推理模型有明显区别于传统LLM的特征：

1. 存在**思考阶段**：显示“Thinking”状态及思考时长（如5s），非即时输出；

2. 推理链完整：拆解问题解决步骤（如计算年龄考虑生日是否过），而非直接给结果；

3. 推理链默认隐藏：仅展示思考后的总结答案，完整推理token不向用户展示。

#### 3.4.2 特殊计费规则

推理模型的**推理token会被计费**，且账单显示token数与用户看到的不一致：

1. OpenAI/Anthropic：按**完整推理token**计费，非仅展示的总结token；推理输出前几行更详细，利于提示工程；

2. Google Gemini：推理token不可见，但占用上下文窗口，按**输出token**计费；

3. 2025.11.04主流模型定价（每100万token）：GPT-5输入 $1.25、输出$ 10.00；Gemini 2.5 Pro输入超20万token后 $2.50，输出超20万token后$ 15.00。

## 四、推理模型的评估体系：基准测试与核心指标

### 4.1 推理模型的基准测试（Benchmark）

评估聚焦**可验证的推理任务**，分为两大核心类别，均要求模型完成“推理链+答案”并通过客观验证：

#### 4.1.1 代码推理

- 任务：解决编程问题、修复bug；

- 代表数据集：HumanEval、CodeForces、SWE-bench；

- 实例：给定小熊尺寸列表，找到比最大小熊小的最大个体，要求编写代码并通过测试用例。

#### 4.1.2 数学推理

- 任务：解决复杂数学问题（如奥数、竞赛题）；

- 代表数据集：AIME、GSM8K；

- 实例：小熊2020年出生，2025年几岁？要求给出计算步骤并验证结果。

### 4.2 核心评估指标：Pass@k

2021年Chen等人提出，为推理/代码模型的经典量化指标，**定义**：在k次生成尝试中，至少有1次成功解决问题的概率。

#### 4.2.1 数学表达式

 $Pass @ k=1-\frac{\binom{n-c}{k}}{\binom{n}{k}}$ 

-  $n$ ：总尝试次数， $c$ ：成功次数， $\binom{a}{b}$ ：组合数（从a中选b个）；

- 特殊情况：Pass@1（单次生成成功的概率），为最基础的评估指标。

#### 4.2.2 衍生指标与适用场景

1. **Cons@k（Consensus at k）**：2025年DeepSeek提出，多数投票的答案与真实答案一致的概率；

2. 场景适配：

    - Pass@k：适用于**检查成本低、可接受高延迟**的场景（如离线代码生成）；

    - Pass@1：适用于**要求单次生成准确、低延迟**的场景（如实时问答）。

#### 4.2.3 影响因素

Pass@k随**采样数k**增加而提升（尝试次数越多，成功概率越高）；**温度系数Temperature**越高（生成越随机），Pass@k提升幅度越明显。

## 五、用强化学习提升推理能力：测试时扩展（Test-time scaling）

### 5.1 核心思路

通过RL**激励模型在测试/推理阶段主动生成推理链**，让模型从“被动响应CoT提示”变为“主动生成CoT”，即**测试时扩展**（非仅训练时提升推理能力）。

### 5.2 设计RL方案的关键考量

需解决传统CoT的落地问题，为RL奖励函数设计提供约束：

1. 推理链人工标注成本极高，**手工构建SFT数据不现实**；

2. 不能将模型限制在**人类编写的推理链**中，需支持模型自主生成高效推理步骤；

3. 奖励信号必须**自然可验证**：通过客观规则判断“模型是否解决问题”（答案对/错，非主观评价）。

### 5.3 核心奖励函数设计：双维度验证

为激励模型生成“有效推理链+正确答案”，设计两个不可分割的奖励维度，最终奖励为二者组合：

1. **奖励1：验证推理链存在（CoT is there）**

模型输出必须包含完整的推理步骤，而非直接输出答案；例：求解数学方程时，需展示平方、整理、推导等步骤。

1. **奖励2：验证答案正确**

通过**测试用例验证**（代码任务）或**数学计算验证**（数学任务）判断答案是否正确，为核心奖励信号。

### 5.4 推理阶段的思维控制策略

不同问题对推理链的长度/深度要求不同，需**动态控制模型的思考过程**，核心思路有4种：

1. **动态预算（Dynamic budget）**：为不同问题分配不同的推理token预算（复杂问题多分配）；

2. **上下文感知（Context awareness）**：模型根据问题上下文自动判断推理深度；

3. **预算强制（Budget forcing）**：强制模型在指定token预算内完成推理；

4. **连续思维（Continuous thoughts）**：让模型在连续隐空间中完成推理，而非离散token生成（Hao et al., 2024）。

## 六、推理模型的核心RL算法：GRPO（Group Relative Policy Optimization）

### 6.1 算法定位与核心目标

由Shao et al.（2024）提出，是**专为推理模型设计的PPO改进算法**，核心目标为在**群体比较**中提升模型推理能力，损失函数延续PPO核心逻辑：

 $\mathscr{L}(\theta)= \text{Maximize advantages} + \text{Don't deviate too much from old/base model}$ 

### 6.2 核心创新：群体相对优势值

GRPO与PPO的最大区别为**优势值的计算方式**，适配推理任务“多推理路径选最优”的特点：

- PPO：优势值 $A_t$ 为**单一样本**的奖励与基准值的差；

- GRPO：优势值为**群体标准化的相对优势值**，公式为：

 $\hat{A}_{i,t}=\frac{r_{i}-mean\left(\{r_1,r_2,\cdots,r_G\}\right)}{std\left(\{r_1,r_2,\cdots,r_G\}\right)}$ 

即单个样本的奖励在一个群体（G为群体大小）中的标准化值，**“和同组比，而非和自己比”**。

### 6.3 GRPO与PPO的完整公式对比

#### 6.3.1 GRPO损失函数

 $\begin{array} {rl}{\mathcal {J}_{GRPO}(\theta )}&{=\mathbb E[q\sim P(Q),\left\{ o_{i}\right\} _{i=1}^{G}\sim \pi _{\theta _{old}}(O|q)]}\\ &{\frac {1}{G}\sum _{i=1}^{G}\frac {1}{\left| o_{i}\right| }\sum _{t=1}^{\left| o_{i}\right| }\left\{ min \left[ \frac {\pi _{\theta }\left( o_{i,t}|q,o_{i,<t}\right) }{\pi _{\theta _{old }}\left( o_{i,t}|q,o_{i,<t}\right) }\hat {A}_{i,t}, clip\left( \frac {\pi _{\theta }(o_{i,t}|q,o_{i,<t}\right) }{\pi _{\theta_{old}}}\left(o_{i,t}|q,o_{i,<t}\right) },1-\varepsilon , 1+\varepsilon \right) \hat {A}_{i,t}\right] -\beta \mathbb {D}_{KL}\left[ \pi _{\theta }\| \pi _{ref}\right] \Bigg \} }\end{array}$ 

#### 6.3.2 PPO损失函数

 $\mathcal{J}_{P P O}(\theta)=\mathbb{E}\left[q \sim P(Q), o \sim \pi_{\theta_{old }}(O | q)\right] \frac{1}{|o|} \sum_{t=1}^{|o|} min \left[\frac{\pi_{\theta}\left(o_{t} | q, o_{<t}\right)}{\pi_{\theta_{old }}\left(o_{t} | q, o_{<t}\right)} A_{t}, clip\left(\frac{\pi_{\theta}\left(o_{t} | q, o_{<t}\right)}{\pi_{\theta_{old }}\left(o_{t} | q, o_{<t}\right)}, 1-\varepsilon, 1+\varepsilon\right) A_{t}\right]$ 

### 6.4 GRPO与PPO的核心异同

#### 6.4.1 相同点

1. 均用**策略比**衡量新策略与旧策略的差异；

2. 均用**clip裁剪**避免策略更新幅度过大；

3. 均以**最大化优势值**为核心训练目标。

#### 6.4.2 不同点

1. **KL惩罚项**：GRPO新增 $-\beta \mathbb{D}_{KL}[\pi_\theta||\pi_{ref}]$ ，用参考模型限制策略偏差，PPO无此项；

2. **优势值**：GRPO用群体标准化相对优势值 $\hat{A}_{i,t}$ ，PPO用单一样本优势值 $A_t$ ；

3. **样本处理**：GRPO处理**群体样本** ${o_i}_{i=1}^G$ ，PPO处理单个样本 $o$ 。

### 6.5 GRPO训练的经典问题：输出长度持续增加

#### 6.5.1 问题现象

随GRPO训练步数增加，模型的平均输出token数大幅上升（如DeepSeek R1-Zero从0增至12000+），但推理准确率提升趋于平缓，模型生成**冗余推理步骤**，推理效率低下。

#### 6.5.2 问题根源

GRPO损失函数中的**token级归一化项** $\frac{1}{|o_i|}$  形成不良激励：

- 短输出： $\frac{1}{|o_i|}$ 值大，单个token的奖励/惩罚被放大，若 $A<0$ ，短输出惩罚更重；

- 长输出： $\frac{1}{|o_i|}$ 值小，单个token的奖励/惩罚被缩小，若 $A<0$ ，长输出惩罚更轻；

模型为规避惩罚，主动生成更长的输出。

#### 6.5.3 解决方案：均衡token级贡献

核心思路：**移除** $\frac{1}{|o_i|}$  **归一化项，让每个token的贡献相等**，两种主流改进方法：

1. **DAPO（Yu et al., 2025）**：按群体总token数归一化

 $\frac{1}{\sum_{i=1}^{G}\left|o_{i}\right|} \sum_{i=1}^{G} \sum_{t=1}^{\left|o_{i}\right|}$ 

1. **Dr. GRPO（Liu et al., 2025）**：直接移除样本级 $\frac{1}{|o_i|}$ ，仅保留群体级 $\frac{1}{G}$ 

 $\frac{1}{G} \sum_{i=1}^{G} \sum_{t=1}^{\left|o_{i}\right|}$ 

#### 6.5.4 其他改进方向

为进一步提升GRPO性能，针对推理任务的优化：

1. **难度偏置调整**：根据问题难度调整优势值计算，避免简单/复杂问题奖励混为一谈；

2. **鼓励推理多样性**：将clip对称区间 $[1-\varepsilon,1+\varepsilon]$ 改为非对称区间 $[1-\varepsilon_{low},1+\varepsilon_{high}]$ ，激励模型生成多样推理链。

## 七、实际推理模型训练案例：DeepSeek R1/R1-Zero

以2025年DeepSeek R1为例，讲解**从基础模型到高性能推理模型的完整训练流程**，核心底座为**DeepSeek V3-Base**（MoE架构，总参671B，激活参37B），分为R1-Zero（概念验证版）和R1（全流程优化版）。

### 7.1 DeepSeek R1-Zero：概念验证版

**核心目标**：无需有监督微调（SFT），仅通过GRPO让模型具备推理能力，训练仅2步：

1. 用传统方法预训练基础模型V3-Base（MoE 671B/37B）；

2. 用推理数据对V3-Base做**GRPO训练**，奖励=格式（推理链存在）+准确率（答案正确）。

#### R1-Zero的优劣势

|优势|劣势|
|---|---|
|无需人工标注SFT数据，训练成本低|推理链格式不规范、可读性差，冗余步骤多|
|直接通过RL获得推理能力，效果显著|-|
### 7.2 DeepSeek R1：全流程优化版

在R1-Zero基础上增加**多阶段SFT+GRPO**，优化推理链规范性与模型泛化能力，共5步：

1. 预训练基础模型V3-Base（同R1-Zero）；

2. **小规模SFT**：用R1-Zero生成的长推理链+人类重写的高质量推理链做微调，让模型学会规范的推理链表达；

3. **第一阶段GRPO**：用推理数据训练，奖励=格式+准确率+语言一致性；

4. **大规模SFT**：融合推理数据（60万对，数学/代码/逻辑）+通用数据（20万对，复用V3 SFT数据）做微调，提升泛化能力；

5. **第二阶段GRPO**：分数据类型训练，最终得到R1模型：

    - 推理数据：奖励=格式+准确率；

    - 通用数据：奖励=有用性+无害性（复用V3 RL数据）。

### 7.3 DeepSeek R1的性能表现

在数学、代码推理上达到行业顶尖水平，通用推理接近OpenAI o1系列，核心亮点：

1. 数学推理：AIME 2024 Pass@1=79.8%、MATH-500 Pass@1=97.3%，远超GPT-4o、Claude 3.5 Sonnet；

2. 代码推理：Codeforces Rating=2029，接近OpenAI o1-1217（2061）；

3. 通用推理：MMLU（Pass@1）=90.8%、DROP（3-shot F1）=92.2%，显著超越基础模型V3；

4. 中文推理：C-Eval（EM）=91.8%、CLUEWSC（EM）=92.8%，适配多语言推理。

## 八、推理模型的蒸馏：小模型的高性能推理落地

### 8.1 蒸馏思路的变革

- **传统LLM蒸馏**：目标为**匹配大模型的下一个token分布**，仅学习生成结果；

- **推理模型蒸馏**：目标为**让小模型学习大模型的推理链（推理轨迹）**，从“token级蒸馏”升级为“推理链级蒸馏”。

### 8.2 蒸馏流程

- 大模型：DeepSeek R1（671B MoE），生成**完整推理链+答案**；

- 小模型：Qwen/Llama系列（1.5B~70B），通过SFT学习R1的推理链，得到**R1-Distill**系列模型。

### 8.3 蒸馏的核心效果

1. **性能有竞争力**：小模型经蒸馏后推理能力大幅提升，如R1-Distill-Qwen-32B的AIME 2024 Pass@1=72.6%，远超GPT-4o（9.3%）、Claude 3.5 Sonnet（16.0%）；

2. **计算效率高**：蒸馏后小模型（32B）推理速度远快于大模型（671B MoE），训练/推理成本大幅降低，适合工程落地；

3. **算力利用高效**：相同算力下，蒸馏模型的推理准确率显著高于同规模原生模型。

> 核心结论
> 1. **思维链（CoT）** 作为推理模型的核心表达范式，通过**显式的步骤化推导**弥补了这一缺陷；而**强化学习**则是推动模型从“被动响应CoT提示”向“主动生成高质量CoT”转变的关键手段，为推理能力的自主习得提供了技术框架。
> 2. **GRPO算法**作为PPO在推理任务中的针对性改进，核心创新在于采用**群体相对优势值**替代传统的单样本绝对优势值。这一设计完美适配了推理任务“**多路径求解**”的特性，通过群体内的横向对比优化策略，既提升了训练稳定性，又有效鼓励了模型对更优推理路径的探索，成为推理模型训练的核心RL算法。
> 3. GRPO训练中出现的**输出长度膨胀**问题，其数学根源是**序列长度归一化项带来的不良激励**。通过**均衡token级贡献**的改进方案（**DAPO/Dr. GRPO**），移除或重构长度相关的归一化逻辑，可在不损失推理准确率的前提下，实现推理链长度的有效控制。
> 4. DeepSeek R1的工业化训练流程，验证了**GRPO+多阶段SFT**混合范式的有效性。其中，SFT负责规范推理链的格式与可读性，解决R1-Zero的逻辑混乱问题；多阶段GRPO则聚焦准确率与场景适配性的精准优化。二者协同，让模型同时具备**推理链规范性**与**任务高准确率**，成为推理模型从技术验证走向商用落地的标杆路径。
> 5. 推理链蒸馏突破了传统“结果模仿”的局限，通过**让小模型学习大模型的完整推理过程**，实现了“轻量化”与“高性能”的平衡。