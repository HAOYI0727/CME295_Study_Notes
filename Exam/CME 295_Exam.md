# CME 295 Exam

## Lecture 1：Transformer background and architecture

### 1. 选择题（1分）
**题目**：相对于词级别的分词，子词方法（如BPE/WordPiece）的一个关键优势是：
A. 消除了对词汇表的需求
**B. 通过学习常见的词干/前缀/后缀减少未登录词（OOV）问题**
C. 无需训练即可使嵌入完全可解释
D. 不需要训练数据

**答案**：B

**解析**：子词分词的核心优势是**拆分词汇为常见子词单元**，能有效处理生僻词、新词等未登录词（OOV），并非消除词汇表、无需训练或让嵌入无训练可解释，因此ACD错误。

### 2. 选择题（1分）
**题目**：在word2vec中，哪个代理任务从上下文预测掩码的中心词？
A. Skip-gram
**B. CBOW**
C. MLM
D. NSP

**答案**：B

**解析**：CBOW（连续词袋模型）的任务是**用上下文词预测中心掩码词**；Skip-gram则是**用中心词预测上下文**；MLM是BERT的掩码语言模型，并非word2vec的任务；NSP是下一句预测任务，也不属于word2vec。

### 3. 选择题（1分）
**题目**：原始RNN处理长距离依赖的核心局限性是：
A. 与注意力机制相比参数过多
**B. 多次连续乘法导致的梯度消失/爆炸**
C. 缺乏隐藏状态
D. 无法处理字符级令牌

**答案**：B

**解析**：RNN按序列逐步计算，梯度回传时需经过多次**矩阵乘法**，易出现**梯度消失（长距离依赖信号丢失）或梯度爆炸（梯度值过大）**，这是其处理长距离依赖的核心问题；RNN参数远少于注意力机制，且拥有隐藏状态，也可处理字符级令牌，ACD错误。

### 4. 选择题（2分）
**题目**：哪个子层出现在解码器栈中，但不出现在编码器栈中？
A. 前馈网络
B. 自注意力
**C. 编码器-解码器（交叉）注意力**
D. 层归一化

**答案**：C

**解析**：Transformer编码器栈包含自注意力、层归一化、前馈网络；解码器栈除了上述模块，还额外包含**编码器-解码器交叉注意力层**，用于关注编码器的输出特征，这是解码器独有的子层。

### 5. 选择题（2分）
**题目**：在缩放点积注意力中，除以$\sqrt{d_k}$主要是为了防止：
A. 通过正则化过拟合
**B. 大的点积导致softmax饱和**
C. 位置信息丢失
D. 梯度检查点的开销

**答案**：B

**解析**：当键（K）和查询（Q）的维度$d_k$较大时，点积$QK^\top$的数值会变得很大，导致softmax函数输入进入**饱和区（梯度趋近于0）**，除以$\sqrt{d_k}$可缩放点积的方差，避免softmax饱和，与正则化、位置信息、梯度检查点无关，ACD错误。

### 6. 选择题（2分）
**题目**：多头注意力的主要优势是：
A. 消除了对FFN层的需求
**B. 并行捕捉多样化的交互模式**
C. 将复杂度降低到线性时间以下
D. 不再需要位置编码

**答案**：B

**解析**：多头注意力将Q、K、V投影到多个不同的子空间，每个头独立计算注意力，能**并行捕捉令牌间不同类型的依赖关系**（如句法、语义交互）；其无法消除FFN层，注意力的时间复杂度仍为$O(n^2d)$（n为序列长度），也不能替代位置编码，ACD错误。

### 7. 选择题（2分）
**题目**：Transformer块中的层归一化主要作用是：
**A. 对每个令牌的隐藏特征（按位置）归一化，以稳定和加速训练**
B. 像BatchNorm一样跨批次和时间维度归一化
C. 比点积更高效地计算注意力分数
D. 消除对残差连接的需求

**答案**：A

**解析**：层归一化（LayerNorm）是**按单个样本的每个令牌维度**做归一化，与批次无关，能有效缓解训练中的**梯度问题**，稳定并加速模型收敛；BatchNorm是跨批次归一化，与LayerNorm不同；LayerNorm不参与注意力分数计算，也无法替代残差连接，BCD错误。

### 8. 选择题（1分）
**题目**：在语言建模的解码器自注意力中，掩码的作用是：
**A. 防止关注未来的令牌**
B. 防止关注过去的令牌
C. 强制均匀的注意力分布
D. 禁用对特殊令牌的注意力

**答案**：A

**解析**：解码器的自注意力是**因果注意力**，需保证语言建模的**自回归特性（根据过去的令牌预测下一个）**，因此通过掩码屏蔽掉序列中当前令牌之后的未来令牌，避免模型提前看到未来信息；BCD均非该掩码的作用。

### 9. 简答题（3+4分）
**题目**：缩放点积注意力。(1) 写出自注意力的核心公式；(2) 简要解释Q、K、V的作用

**答案**：
1. $Attention (Q, K, V)=softmax\left(\frac{Q K^{\top}}{\sqrt{d_{k}}}\right) V$
2. $Q$为**查询**矩阵，代表当前令牌想要“查询”的信息；$K$为**键**矩阵，代表序列中所有令牌提供的“键”信息，$Q$与$K$的点积用于计算**令牌间的注意力分数**；$V$为**值**矩阵，代表序列中所有令牌的**实际特征信息**，注意力权重会对V进行**加权求和**，得到自注意力的输出。

**解析**：自注意力的核心是通过**Q-K的相似性**计算注意力权重，再**对V加权**，除以$\sqrt{d_k}$避免softmax饱和，softmax将注意力分数**归一化**为0-1的权重分布。

### 10. 简答题（3+3分）
**题目**：标签平滑。(1) 总结标签平滑的优化目标；(2) 说明其为何有助于泛化

**答案**：
1. **标签平滑**是一种正则化技术，将独热标签替换为**跨类别的平滑概率分布**，优化目标是让模型不再对标签做出绝对的预测，而是**学习类别间更细微的特征表示，缓解过拟合**。
2. **独热标签**会让模型过度自信地**拟合训练数据**的标签，甚至记住噪声；**标签平滑**通过给非目标类别分配少量概率，迫使模型**学习更通用的特征表示**，减少对训练数据噪声的依赖，从而**提升模型在测试集上的泛化能力**。

**解析**：标签平滑的本质是**降低模型对训练标签的“过度拟合”**，让模型的预测分布更平缓，提升鲁棒性。

## Lecture 2：Transformer-based models & tricks

### 1. 选择题（1分）
**题目**：正弦（硬编码）位置编码的一个优势是：
A. 扩展到更长序列时需要重新训练
**B. 无需重新训练即可实现长度外推**
C. 通过学习的每头偏置直接编码相对位置
D. 消除对令牌嵌入的需求

**答案**：B

**解析**：正弦位置编码是基于数学公式的**硬编码方式**，不依赖训练，因此当输入序列长度超过训练时的长度时，可直接计算新长度的位置编码，实现**长度外推**；Learned位置编码才需要重新训练，正弦PE不编码相对位置，也无法替代令牌嵌入，ACD错误。

### 2. 选择题（2分）
**题目**：BERT的预训练目标主要使用：
**A. 掩码语言建模（MLM）**
B. 自回归的下一个令牌预测
C. 仅在解码器进行序列到序列的去噪
D. 无掩码的下一句生成

**答案**：A

**解析**：BERT是编码器仅有的模型，核心预训练目标为**掩码语言建模（MLM）** 和**下一句预测（NSP）**，其中**MLM**是主要目标；自回归下一个令牌预测是GPT等解码器仅有的模型目标，解码器的去噪、无掩码NSP均非BERT的预训练方式，BCD错误。

### 3. 选择题（2分）
**题目**：哪种架构在预训练时是双向的，且适合句子编码/分类？
A. 解码器仅有的模型（如GPT）
B. 编码器-解码器模型
**C. 编码器仅有的模型（如BERT）**
D. 仅循环神经网络

**答案**：C

**解析**：**编码器仅有**的模型（BERT）采用**双向自注意力**，能捕捉整个序列的上下文信息，非常适合句子编码、文本分类等理解类任务；**解码器仅有**的模型是**单向因果注意力**，适合生成类任务；**编码器-解码器**模型适合**seq2seq生成**（如翻译）；RNN的双向版本虽能捕捉双向信息，但效果和效率远不如Transformer编码器，ABD错误。

### 4. 选择题（2分）
**题目**：旋转位置编码（RoPE）的主要作用是：
A. 将正弦/余弦向量添加到令牌嵌入中
B. 屏蔽对长距离令牌的注意力
C. 按距离均匀降低值的权重
**D. 通过位置相关的2D块旋转Q和K，编码相对距离**

**答案**：D

**解析**：RoPE的核心思想是**将位置信息编码到Q和K的旋转操作中**，通过位置相关的**2D旋转矩阵**对Q、K进行旋转，使注意力分数仅依赖于令牌间的**相对距离**，而非绝对位置；A是正弦位置编码的做法，BC并非RoPE的功能，错误。

### 5. 选择题（1分）
**题目**：如今大语言模型（LLM）中最常用的位置信息编码方法是：
**A. 在注意力内部应用RoPE**
B. 学习的绝对位置嵌入添加到令牌中
C. 仅将正弦PE添加到令牌中
D. 仅将相对位置偏置添加到值中

**答案**：A

**解析**：**RoPE能自然编码相对位置，且支持长度外推**，是当前LLM（如LLaMA、GPT-2/3）中**最主流**的位置编码方式；学习的绝对位置嵌入不支持长度外推，正弦PE表达能力有限，仅对V添加相对位置偏置的方式极少使用，BCD错误。

### 6. 选择题（2分）
**题目**：Longformer主要通过哪种方式降低注意力的计算成本？
A. 仅对注意力矩阵进行低秩分解
**B. 滑动窗口局部注意力加可选的全局令牌**
C. 消除键以节省内存
D. 用卷积替换注意力

**答案**：B

**解析**：Longformer针对长序列，将标准的全注意力替换为**滑动窗口局部注意力**（每个令牌仅关注周围窗口内的令牌），同时支持设置**全局令牌**，使其能关注整个序列，大幅降低注意力的计算成本（从$O(n^2)$降至$O(n)$）；ACD均非Longformer的优化方式，错误。

### 7. 选择题（1分）
**题目**：在因果自注意力中，掩码应用于：
**A. softmax之前的$Q K^\top$分数矩阵**
B. softmax之后的值矩阵V
C. 注意力块之前的令牌嵌入
D. 最终线性层之后的输出logits

**答案**：A

**解析**：因果掩码是**在计算softmax之前**，对$QK^\top$分数矩阵的**上三角部分**（未来令牌的分数）设置为负无穷，这样softmax后这些位置的权重会趋近于0，从而实现**屏蔽未来令牌**；掩码不作用于V、令牌嵌入或最终logits，BCD错误。

### 8. 选择题（1分）
**题目**：注意力块之间的前馈网络（FFN）子层的主要目的是：
A. 聚合序列顺序
B. 减少注意力头的数量
C. 替代位置编码
**D. 提供令牌级的非线性和通道混合**

**答案**：D

**解析**：FFN是**令牌级的全连接网络**，对每个令牌的特征独立进行**非线性变换**和**通道维度的升维/降维（混合）**，提升模型的非线性表达能力；序列顺序由**位置编码和注意力**捕捉，FFN不涉及注意力头数量调整，也无法替代位置编码，ABC错误。

### 9. 简答题（3+2+2分）
**题目**：模型家族。比较编码器仅、编码器-解码器、解码器仅的Transformer家族：(1) 各自典型的预训练目标；(2) 各自的一个模型示例；(3) 如今大语言模型的实际默认架构

**答案**：
1. 编码器仅：**掩码语言建模（MLM）**；编码器-解码器：**跨度损坏预测（Span corruption）**；解码器仅：**自回归的下一个令牌预测**。
2. 编码器仅：**BERT**；编码器-解码器：**T5**；解码器仅：**GPT**。
3. **解码器仅**。

**解析**：**解码器仅**的Transformer采用自回归生成方式，更适合大语言模型的**自然语言生成**核心需求，且训练和推理的工程优化更成熟，成为当前LLM的主流架构；**编码器仅**适合**理解类**任务，**编码器-解码器**适合**seq2seq生成**任务。

### 10. 简答题（4+2分）
**题目**：RoPE的直观理解。(1) 旋转Q和K如何让注意力依赖于相对位置？(2) 说出一个实际优势

**答案**：
1. RoPE通过**位置相关的2D旋转矩阵**对不同位置的Q和K进行旋转操作，使得两个令牌的Q、K点积结果仅由它们的**相对位置**决定，而非绝对位置；当两个令牌的相对距离不变时，无论其绝对位置如何，注意力分数保持一致，从而**让注意力机制捕捉到相对位置信息**。
2. 支持**长度外推**，无需重新训练即可处理比训练时更长的序列（实际优势也可答：注意力分数的计算仅依赖**相对距离**，更符合**自然语言的位置特征**）。

**解析**：RoPE解决了传统绝对位置编码无法外推长序列的问题，且**相对位置编码**更贴合自然语言中“**语义依赖由相对距离决定**”的特性。

## Lecture 3：Large Language Models

### 1. 选择题（1分）
**题目**：在本课程中，LLM最恰当的描述是：
A. 用MLM训练的编码器仅分类器
B. 用教师强制训练的seq2seq模型
**C. 带掩码自注意力的解码器仅自回归下一个令牌预测器**
D. 带局部注意力的双向LSTM语言模型

**答案**：C

**解析**：当前主流LLM均为**解码器仅的Transformer架构**，核心是**带因果掩码的自注意力**，预训练目标为**自回归的下一个令牌预测**；A是BERT类模型，B是T5类seq2seq模型，D是传统RNN模型，均非LLM的定义，错误。

### 2. 选择题（1分）
**题目**：在稀疏MoE LLMs中，路由的工作方式是：
**A. 为每个令牌激活学习到的子集（如前k个专家）**
B. 激活所有专家并平均输出
C. 仅在预训练时使用专家
D. 随机均匀选择专家

**答案**：A

**解析**：稀疏MoE（混合专家）模型将模型分为**多个“专家”子网络**，**路由网络**为每个令牌计算各专家的**权重**，仅激活**权重最高的前k个专家**（稀疏激活），大幅减少计算量；激活所有专家是稠密MoE，MoE在预训练和微调均会使用，随机选择无学习意义，BCD错误。

### 3. 选择题（2分）
**题目**：PagedAttention的主要目标是：
A. 通过剪枝层减少预训练的浮点运算量（FLOPs）
B. 用卷积替换注意力
C. 通过梯度检查点用更大的批次训练
**D. 用分页管理KV缓存内存，缓解碎片问题并提升服务效率**

**答案**：D

**解析**：PagedAttention是针对LLM推理时**KV缓存**的内存优化技术，借鉴操作系统的分页思想，**将KV缓存划分为固定大小的页，为每个序列分配离散的页**，缓解内存碎片问题，提升内存利用率和服务的**吞吐量/效率**；与剪枝、卷积、梯度检查点无关，ABC错误。

### 4. 选择题（2分）
**题目**：推测解码（Speculative decoding）通过哪种方式加速生成？
A. 用更大的束宽运行束搜索
**B. 用小型草稿模型提议令牌，由目标模型验证/修正**
C. 在线量化目标模型
D. 移除softmax温度

**答案**：B

**解析**：推测解码的核心是**双模型协作**：用**轻量、快速的小型草稿模型**快速生成**多个候选令牌**，再将候选序列输入大的**目标模型**进行一次性验证，**仅修正错误的令牌**，避免目标模型逐令牌生成的耗时，大幅提升生成速度；更大束宽会增加计算量，在线量化、移除温度均非推测解码的原理，ACD错误。

### 5. 选择题（1分）
**题目**：推理时KV缓存的目的是：
**A. 避免重新计算过去的键/值，减少每个令牌的延迟**
B. 存储反向传播的梯度
C. 在步骤间保存优化器状态
D. 不增加计算量的前提下提升参数数量

**答案**：A

**解析**：LLM推理是自回归的，每生成一个新令牌，过去令牌的K、V不会改变，**KV缓存会保存这些已计算的K、V，避免重复计算**，将每个令牌的注意力计算复杂度从$O(n^2)$降至$O(n)$，大幅降低单令牌生成延迟；KV缓存不存储梯度和优化器状态，也与参数数量无关，BCD错误。

### 6. 选择题（2分）
**题目**：与MHA相比，MQA/GQA主要通过哪种方式降低延迟？
A. 跨头共享Q
B. 完全移除缓存
**C. 跨头（或分组）共享K/V，缩小KV缓存/带宽**
D. 将注意力头的数量翻倍

**答案**：C

**解析**：MHA（多头注意力）中每个头有独立的K/V，KV缓存的大小与头数成正比；**MQA（多查询注意力）**让所有头共享一组K/V，**GQA（分组查询注意力）**将头分为若干组，每组共享一组K/V，二者均大幅**减小KV缓存的体积**，降低内存带宽开销，从而减少推理延迟；MQA/GQA不共享Q，也不会移除缓存，头数翻倍会增加延迟，ABD错误。

### 7. 选择题（2分）
**题目**：在核采样（top-p）中，下一个令牌从哪里采样？
A. 概率最高的前k个令牌
B. 所有概率高于固定阈值τ的令牌
**C. 累积概率质量≥p的最小令牌集合**
D. 单个概率最高的令牌

**答案**：C

**解析**：**核采样（top-p）**是**动态阈值**采样方式：将令牌按概率**从高到低排序**，累加概率直到和≥p，取这个过程中的所有令牌构成候选集，再从候选集中采样；A是top-k采样，B是固定阈值采样，D是贪心搜索，均非top-p采样，错误。

### 8. 选择题（1分）
**题目**：解码时提高softmax温度T通常会：
**A. 拉平分布（增加多样性）**
B. 锐化分布（减少多样性）
C. 对概率无影响
D. 强制束搜索行为

**答案**：A

**解析**：softmax温度T的作用是缩放logits：$softmax(z/T)$，当**T增大**时，logits的差异被缩小，softmax后的概率分布更**平缓**，低概率令牌的采样概率提升，生成结果的**多样性增加**；T减小时分布锐化，多样性降低，BCD错误。

### 9. 简答题（4+3分）
**题目**：路由坍塌。(1) 定义稀疏MoE训练中的“路由坍塌”；(2) 给出一种标准的缓解方法

**答案**：
1. **路由坍塌**是指在**稀疏MoE模型**的训练过程中，路由网络始终将所有（或绝大多数）令牌路由到**同一组/少数几个专家**，其余专家几乎从未被激活，导致**专家的能力无法充分利用**，模型性能下降。
2. 添加**辅助损失**（如**负载均衡损失**），鼓励路由网络将令牌**均匀分配**到各个专家，避免少数专家被过度使用（缓解方法也可答：对专家的输入进行**归一化**、限制每个专家的**最大令牌数**等）。

**解析**：路由坍塌是稀疏MoE训练的核心问题，本质是**路由网络的优化陷入局部最优**，导致专家利用不均。

### 10. 简答题（3+2+1分）
**题目**：解码的权衡。从(1)多样性、(2)质量、(3)计算量三个方面比较贪心/束搜索与top-k/top-p采样

**答案**：
1. **多样性**：贪心/束搜索多样性**低**；top-k/top-p采样多样性更**高**。
2. **质量**：贪心搜索生成质量**较低**；束搜索生成质量**较高**；top-k/top-p采样生成质量**较高**。
3. **计算量**：贪心搜索计算量**最小**；束搜索计算量**较大**（与束宽正相关）；top-k/top-p采样计算量**较小**。

**解析**：**贪心/束搜索**是**确定性的解码方式**，追求“**最优**”令牌，因此多样性低，束搜索因遍历更多候选导致计算量增大；**top-k/top-p**是**随机采样方式**，能引入随机性提升多样性，且采样的计算量远小于束搜索，同时合理的采样参数能保证生成质量。

## Lecture 4：LLM training

### 1. 选择题（1分）
**题目**：解码器仅LLMs的标准预训练目标是：
A. 掩码语言建模
B. 下一句预测
**C. 自回归的下一个令牌预测**
D. 序列自编码

**答案**：C

**解析**：解码器仅LLMs的核心预训练目标是**自回归下一个令牌预测**，即根据前文的令牌序列，预测下一个最可能的令牌；A是BERT的目标，B是BERT的辅助目标，D是自编码器的目标，均非解码器仅LLM的预训练目标，错误。

### 2. 选择题（1分）
**题目**：现代LLMs中典型的预训练数据混合包括：
A. 仅监督的指令-响应对
**B. 大规模的网络爬取文本和代码**
C. 仅音频转录文本
D. 纯合成令牌

**答案**：B

**解析**：现代LLMs的预训练数据需要**大规模、多样化**的文本数据，核心包括网络爬取的**通用文本（网页、书籍、文章）和代码（GitHub等）**，以提升模型的通用表达和推理能力；指令-响应对用于微调（非预训练），仅音频转录文本数据量不足，纯合成令牌缺乏真实语义，ACD错误。

### 3. 选择题（1分）
**题目**：监督微调（SFT）最恰当的总结是：
A. 冻结模型仅添加适配器
**B. 收集指令/响应对，微调模型以改变其行为**
C. 训练奖励模型
D. 用PPO进行强化学习

**答案**：B

**解析**：**监督微调（SFT）**是LLM**对齐**的第一步，通过收集**人工标注的指令-响应对**，在预训练好的LLM上进行**微调**，让模型学习**遵循人类指令**，生成符合人类预期的回答；A是参数高效微调的方式，C是RLHF的第二步，D是RLHF的第三步，均非SFT的定义，错误。

### 4. 选择题（1分）
**题目**：用于参数高效微调的低秩适配（LoRA）的工作方式是：
A. 剪枝注意力头
B. 量化激活值
C. 仅训练嵌入层
**D. 在权重更新中添加低秩适配器矩阵，同时冻结基础模型权重**

**答案**：D

**解析**：LoRA的核心是**冻结预训练模型的所有基础权重**，在注意力层的权重矩阵旁添加**低秩的A、B矩阵**，仅训练这两个低秩矩阵，**通过低秩矩阵的更新近似替代原权重的全秩更新**，大幅减少微调的参数量；剪枝、量化、仅训练嵌入层均非LoRA的做法，ABC错误。

### 5. 选择题（2分）
**题目**：实际中用于LLMs的混合精度训练通常：
A. 将所有张量存储为FP64，避免数值误差
B. 前向传播用FP64，反向传播用INT8
C. 需要修改模型架构
**D. 对激活值/梯度使用低精度，同时保留高精度的权重副本用于更新**

**答案**：D

**解析**：混合精度训练的核心是**分精度存储/计算**：用**FP16（低精度）**进行**前向和反向传播**的计算（激活值、梯度），减少内存占用和计算量；同时保留**FP32（高精度）**的模型权重副本，用于**梯度更新**，避免低精度导致的数值不稳定；FP64计算量和内存占用过大，混合精度无需修改模型架构，ABC错误。

### 6. 选择题（2分）
**题目**：FlashAttention的主要优化是：
**A. 通过分块到片上SRAM和选择性重计算最小化HBM（GPU显存）的读写，同时保持注意力计算的精确性**
B. 用低秩投影近似softmax注意力
C. 用卷积替换注意力以减少FLOPs
D. 通过填充令牌增加序列长度

**答案**：A

**解析**：FlashAttention是**注意力的工程实现优化**，核心是**利用GPU的片上SRAM（高速内存），将Q、K、V分块后逐块计算注意力**，同时通过**选择性重计算**避免存储中间结果，大幅减少对低速HBM显存的读写，且整个过程保持注意力计算的**精确性**（非近似）；低秩近似、卷积替换、填充令牌均非FlashAttention的优化方式，BCD错误。

### 7. 选择题（2分）
**题目**：在带ZeRO的数据并行训练中，将优化器状态、梯度和参数跨设备分片的变体是：
A. ZeRO-1
B. ZeRO-2
**C. ZeRO-3**
D. 以上都不是

**答案**：C

**解析**：ZeRO是数据并行的内存优化技术，分三个等级：
- ZeRO-1：仅对**优化器状态**跨设备分片；
- ZeRO-2：对**优化器状态+梯度**跨设备分片；
- ZeRO-3：对**优化器状态+梯度+模型参数**全部分片，是内存优化程度最高的变体。

### 8. 选择题（2分）
**题目**：本课程中讨论的QLoRA主要：
**A. 将冻结的基础权重以低位格式存储（4位NF4，带双量化），同时以更高精度训练小的LoRA适配器；矩阵乘法以更高精度执行**
B. 将权重和激活值均量化为1位，并训练整个模型
C. 无需适配器，通过所有参数的FP32副本进行全反向传播
D. 仅压缩优化器状态，保持权重不变

**答案**：A

**解析**：QLoRA是LoRA的量化版本，核心是**低位量化基础模型+高精度训练LoRA**：将预训练的基础模型**权重冻结并量化为4位NF4格式**（还引入**双量化**进一步压缩），仅训练**高精度的LoRA适配器**，矩阵乘法时将低位权重反量化为**高精度执行**，在几乎不损失性能的前提下，大幅降低微调的内存占用；1位量化会导致性能严重下降，QLoRA需要适配器，也非仅压缩优化器状态，BCD错误。

### 9. 简答题（3+4分）
**题目**：指令微调。(1) 描述与预训练模型相比较，指令微调试图实现的目标；(2) 列出课堂上讨论的两个实际挑战

**答案**：
1. 语言能力迁移到各种具体的**下游任务**中，无需为每个任务单独微调，**提升模型的零样本/少样本性能，实现模型对齐**。
2. 实际挑战：需要**高质量的指令-响应对数据**，人工标注成本高；模型性能对**微调数据集的选择和分布**非常敏感，数据集偏置会导致模型行为异常（挑战也可答：指令的多样性不足导致模型泛化能力差、微调过程中易出现灾难性遗忘等）。

**解析**：预训练模型仅具备**通用的语言建模能力**，无法直接遵循人类指令完成具体任务，**指令微调是实现模型“对齐人类意图”的关键步骤**。

### 10. 简答题（4+2分）
**题目**：FlashAttention。(1) 简要解释FlashAttention的核心思想；(2) 说明一个实际中观察到的好处

**答案**：
1. FlashAttention的核心思想是**重新设计注意力的计算流程**，结合**分块（Tiling）** 和**选择性重计算（Selective Recomputation）** 技术，**利用GPU的高速片上SRAM替代低速的HBM显存进行中间计算**，最小化HBM的读写操作，同时通过**重计算**避免存储大量中间注意力分数，在保持注意力计算**精确性**的前提下**优化内存和速度**。
2. 实际好处：大幅**降低注意力计算的内存占用**，支持训练/推理更长的序列；同时提升注意力的计算速度，**减少训练和推理的耗时**（二者答其一即可）。

**解析**：FlashAttention是注意力的底层实现优化，而非算法层面的修改，**是大序列长度LLM训练和推理的关键技术**。

## Lecture 5：LLM tuning

### 1. 选择题（1分）
**题目**：标准监督微调（SFT）的一个关键局限性，是偏好微调（如RLHF）专门设计来解决的？
A. SFT本质上阻止模型理解复杂指令遵循或输出格式化所需的结构依赖
**B. SFT在优质样本上最大化似然，但缺乏显式机制惩罚不良/有害输出（提供负向信号）**
C. 由于需要标注数据，SFT的计算成本比强化学习高指数级
D. SFT会导致灾难性遗忘，严格阻止模型泛化到训练中未见过的任何提示

**答案**：B

**解析**：SFT仅利用**人工标注的优质指令-响应对**做监督训练，仅对优质输出做正向优化，**无针对不良/有害输出的负向惩罚机制**；RLHF通过人类偏好数据构建**奖励模型**，能对输出做**优劣区分**，解决该问题。A中SFT可学习结构依赖和格式，C中SFT计算成本远低于RL，D中SFT不会导致完全的灾难性遗忘，均错误。

### 2. 选择题（1分）
**题目**：标准RLHF流程中的奖励模型（RM）将什么作为输入？
A. 仅提示词（prompt）
B. 仅回复（response）
**C. 一个提示词-回复对**
D. 两个相互竞争的模型回复（无初始提示词的条件）

**答案**：C

**解析**：奖励模型的核心是评估**特定提示词下**模型回复的质量，因此输入必须是**提示词和对应的回复对**，单独的提示词或回复无法完成质量评估；D是人类偏好标注的输入形式（对比两个回复），而非奖励模型的直接输入，错误。

### 3. 选择题（2分）
**题目**：Bradley-Terry公式在奖励建模中用于？
A. 自回归生成高质量合成文本数据，扩充SFT训练集
B. 精确计算参考模型分布与当前策略之间的KL散度
C. 动态优化近端策略优化（PPO）算法的学习率调度
**D. 基于回复的奖励差值，建模回复A优于回复B的概率**

**答案**：D

**解析**：Bradley-Terry是**成对比较**的概率模型，核心是通过**两个对象的评分差值**，计算一个对象优于另一个的概率，在RLHF中用于将**人类的成对偏好标注**转化为**奖励模型的训练信号**，建模回复A比回复B好的概率；ABC均非该公式的用途，错误。

### 4. 选择题（2分）
**题目**：在PPO目标函数中，项$-\beta KL(\pi_{\theta}(y | x)|| \pi_{ref }(y | x))$的加入是为了？
**A. 防止模型与基础模型偏离过远**
B. 不加区分地最大化原始奖励信号，优先考虑高分而忽略文本的语义连贯性或质量
C. 强制模型严格记忆训练数据，确保推理时无幻觉
D. 显著增加生成回复的熵和多样性，防止模式坍塌

**答案**：A

**解析**：该KL散度项是**正则化项**，衡量当前策略模型$\pi_\theta$与参考模型（通常是SFT后的模型）$\pi_{ref}$的分布差异，乘以系数$\beta$并取负后，会**惩罚两者差异过大**的情况，**防止模型为了追求高奖励而生成语义无意义的内容，偏离基础模型的语言能力**；B与该正则化的目的相反，C中该项无强制记忆的作用，D中熵增并非该项的目标，均错误。

### 5. 选择题（2分）
**题目**：直接偏好优化（DPO）的核心理论见解是？
A. 无需任何偏好数据，即可通过强化学习直接训练奖励模型
B. 对于所有任务，PPO在数学上比标准监督学习方法更简单、更稳定
**C. 最优策略可通过解析求解，允许通过监督损失进行隐式的奖励优化**
D. 若监督微调的数据集规模超过某个阈值，奖励模型将完全冗余

**答案**：C

**解析**：DPO的核心理论突破是推导出**最优策略的解析解**，将RLHF中“**训练奖励模型+PPO强化学习**”的两步流程，转化为**直接利用人类偏好数据的监督损失优化**，**无需显式训练奖励模型和价值函数**；A中DPO仍需要偏好数据，B中PPO比监督学习更复杂，D中数据集规模无法让奖励模型冗余，均错误。

### 6. 选择题（2分）
**题目**：Best-of-N（BoN）是强化学习训练的替代方法，其工作方式是？
A. 生成单个回复，仅当对数概率超过预定义的置信阈值时才接受
B. 招募N名人类标注者，为每个查询手动编写和筛选最佳回复
C. 对N个不同微调模型的突触权重取平均，生成鲁棒的集成预测
**D. 生成N个回复，选择奖励模型评分最高的那个**

**答案**：D

**解析**：BoN是一种**推理阶段的优化方法**，核心是利用模型生成**多个（N个）候选回复**，通过**奖励模型**对这些回复**打分**，选择**分数最高**的作为最终输出，无需额外的强化学习训练；A是单样本阈值筛选，B是人类标注，C是模型集成，均非BoN的做法，错误。

### 7. 选择题（1分）
**题目**：RLHF中“奖励黑客（reward hacking）”的常见症状是？
**A. 模型生成奖励值高但实际质量低的输出**
B. 模型进入退化状态，在提示词后完全停止生成任何令牌
C. 训练损失函数在最初几个优化步骤内瞬间降至绝对零
D. 模型完全丧失预训练的解析外部函数调用或使用工具的能力

**答案**：A

**解析**：**奖励黑客**指模型利用奖励函数的设计缺陷，生成**在奖励指标上表现优异，但不符合人类实际需求的低质量输出**（如无意义的长文本、重复内容）；BCD均非奖励黑客的典型表现，错误。

### 8. 选择题（1分）
**题目**：在PPO中，价值函数的作用是估计？
A. 序列中下一个令牌在词汇表上的即时概率分布
**B. 从当前令牌序列开始的预期未来奖励**
C. 当前策略权重与旧策略权重之间的统计差异（散度）
D. 奖励模型在验证集上的二分类准确率

**答案**：B

**解析**：PPO中的价值函数（Critic）是**状态价值估计器**，核心作用是对**当前的令牌序列（状态）** 进行评分，估计从该状态出发，模型后续生成的序列能获得的**累计未来奖励**，用于计算**优势函数（Advantage）**；A是策略模型（Actor）的输出，C是KL散度的计算目标，D与价值函数无关，均错误。

### 9. 简答题（3+4分）
**题目**：RLHF流程。(1) 列出SFT阶段后，RLHF涉及的两个主要训练阶段；(2) 简要解释第一个阶段训练的模型的具体作用/输出。

**答案**：
1. 奖励模型训练（Reward modeling）；强化学习训练（PPO）。
2. 第一阶段训练的奖励模型以**提示词-回复对**为输入，输出一个**标量分数**，该分数代表该回复在**人类偏好**层面的质量（如相关性、有用性、安全性）。

**解析**：RLHF的核心是**将人类偏好转化为模型的奖励信号**，第一步先通过人类成对偏好数据训练**奖励模型**，实现对回复质量的量化评分；第二步再以奖励模型的评分为奖励信号，**用PPO对SFT模型做强化学习微调**。

### 10. 简答题（3+3分）
**题目**：DPO与PPO。(1) 解释训练过程中DPO相对于PPO的主要架构优势（考虑加载的模型数量）；(2) 说明为何仍可能选择PPO而非DPO的一个原因。

**答案**：
1. DPO的**内存效率更高**，因为它无需加载单独的奖励模型和价值函数，仅需加载**策略模型和参考模型**（通常为SFT后的模型），大幅减少了训练时的内存占用。
2. PPO支持**不可微或稀疏的奖励信号**（如代码编译器的运行结果、数学验证器的正误判断），这类信号无法被DPO所需的**静态成对偏好数据集**捕捉，而DPO仅适用于有**成对偏好标注**的场景。

**解析**：**DPO**通过数学推导简化了RLHF的训练流程，减少了模型加载数量；但**PPO**的灵活性更高，可对接各类非结构化的奖励信号，这是其不可替代的优势。

## Lecture 6：LLM reasoning

### 1. 选择题（1分）
**题目**：在LLM的语境中，“推理（reasoning）”最恰当的定义是？
A. 准确检索并复述预训练中见过的特定事实字符串的能力
B. GPU上每秒生成令牌的计算吞吐量速度
**C. 解决复杂问题的能力，通常通过多步逻辑推理实现**
D. 遵循格式化指令的能力

**答案**：C

**解析**：LLM中的推理指模型利用已学知识，通过**多步逻辑推导**解决复杂问题的能力（如数学计算、逻辑推理、分步规划）；A是事实记忆与检索，B是推理速度而非推理能力，D是指令遵循能力，均非推理的核心定义，错误。

### 2. 选择题（1分）
**题目**：思维链（Chain of Thought, CoT）的核心思想是？
A. 强制模型立即输出最终答案，以最小化令牌使用和延迟
B. 仅在高保真的百科全书中对模型进行微调，以提高事实密度
**C. 提示模型在生成最终答案前，解释其思考过程**
D. 利用外部向量数据库检索系统，直接从语料库中获取答案

**答案**：C

**解析**：CoT是一种提示工程方法，核心是**通过示例引导模型**，在输出最终答案前，生成**分步的推理过程**（即“**思维链**”），将复杂问题拆解为简单子问题，提升模型的复杂推理能力；A与CoT的核心相反，B是事实知识微调，D是检索增强生成（RAG），均错误。

### 3. 选择题（2分）
**题目**：在DeepSeek R1-Zero中，观察到模型在RL训练过程中输出长度有什么现象？
**A. 随着模型生成更多推理步骤，输出长度自然增加**
B. 随着模型学习得更简洁高效，输出长度持续减少
C. 无论推理任务的复杂度如何，输出长度在统计上保持不变
D. 输出长度随机波动，与奖励信号或任务难度无相关性

**答案**：A

**解析**：DeepSeek R1-Zero是专注于**推理**的LLM，在RL训练中，模型会逐渐学会生成**更多的分步推理步骤**来解决复杂问题，因此输出长度会自然增加；BCD均与该实际观察结果不符，错误。

### 4. 选择题（2分）
**题目**：组相对策略优化（GRPO）与PPO的主要区别是？
A. 它使用一个复杂度显著更高的神经价值函数，需要单独的预训练
B. 它用GAN风格的判别器网络完全替代奖励模型
C. 它需要在训练的每一个优化步骤中获取实时人类反馈
**D. 它无需价值函数，通过一组输出的平均奖励来估计优势函数**

**答案**：D

**解析**：GRPO是PPO针对推理模型的改进版本，核心创新是**移除了单独的价值函数**，通过对**同一提示词生成的一组（group）输出**计算**奖励的均值和标准差**，对单个输出的奖励做**标准化**，从而**估计优势函数**；A中GRPO无复杂价值函数，B中未替代奖励模型，C中无需实时人类反馈，均错误。

### 5. 选择题（2分）
**题目**：以下哪项被视为训练推理模型的“可验证奖励（verifiable reward）”？
A. 关于生成语气的“礼貌性”或“有用性”的主观人类评分
**B. 代码是否通过单元测试，或数学答案是否与真实标签匹配**
C. 从预训练语料库分布中导出的标准困惑度分数
D. 生成回复的令牌总数，对更长的输出进行奖励

**答案**：B

**解析**：可验证奖励指**客观、可量化、能通过外部工具验证**的奖励信号，代码通过**单元测试**、数学答案**匹配真实值**均是明确的二值结果，无主观偏差；A是主观奖励，C是语言建模的评价指标，与推理质量无关，D是对长度的简单奖励，易导致奖励黑客，均错误。

### 6. 选择题（1分）
**题目**：在DeepSeek R1Zero等推理模型中观察到的“顿悟时刻（aha moment）”是指？
A. 模型突然过拟合，记忆了整个评估数据集的精确解决方案
B. 训练损失函数在数学上收敛到零的精确迭代步骤
C. 模型严格拒绝回答任何其认为可能不安全或有争议的问题
**D. 模型学会自我评估，并在思考过程中回溯/纠正自己的错误**

**答案**：D

**解析**：推理模型的“顿悟时刻”指模型在RL训练中，逐渐学会**自我反思和错误纠正**的能力：生成推理步骤后会**自我评估是否正确**，若发现错误则回溯重新推导，而非直接输出错误答案；ABC均非该概念的含义，错误。

### 7. 选择题（2分）
**题目**：为何在推理模型的RL训练前，通常会使用“冷启动（小规模高质量数据的SFT）”？
A. 教授基础模型预训练中缺失的基础英语语法规则
B. 强制限制模型在RL阶段使用过多的GPU计算资源
**C. 稳定输出格式，让推理轨迹对人类可读**
D. 人为将模型的上下文窗口大小扩展到其架构限制之外

**答案**：C

**解析**：推理模型需要生成**结构化的推理轨迹**（如分步计算、逻辑推导），冷启动的小规模高质量SFT会**引导模型学习统一的推理输出格式**，让推理轨迹清晰、人类可读，同时为后续RL训练奠定稳定的基础；A中基础模型已掌握语法规则，B中无限制计算资源的作用，D中无法突破架构的上下文窗口限制，均错误。

### 8. 选择题（1分）
**题目**：在推理模型的语境中，“蒸馏（distillation）”指的是？
A. 利用量化技术将模型的浮点权重压缩为4位整数
**B. 利用强模型（如R1）生成的推理轨迹，对更小/更弱的模型做SFT**
C. 在推理阶段系统性地剔除所有推理令牌，以加快生成速度
D. 仅基于人类偏好标签训练一个单独的奖励模型

**答案**：B

**解析**：推理模型的蒸馏是**知识蒸馏**的一种，核心是将**强推理模型**生成的高质量推理轨迹（包含分步思考过程）作为**监督数据**，对**更小、更轻量的弱模型**做SFT，**让弱模型继承强模型的推理能力**；A是模型量化，C是推理轨迹的裁剪，D是奖励模型训练，均非蒸馏的含义，错误。

### 9. 简答题（4+2分）
**题目**：GRPO机制。(1) 解释在GRPO中，一组G个输出中，单个输出$O_{i}$的优势函数如何计算；(2) 说明该方法相较于标准PPO的一个计算优势。

**答案**：
1. **优势函数**通过**对组内的奖励做标准化**计算得到。对于单个输出$O_i$，其奖励为$r_i$，则优势函数$A_i=\frac{r_{i}-mean(r)}{std(r)}$，其中$mean(r)$和$std(r)$是对该组G个输出的奖励计算的**均值和标准差**。
2. 该方法移除了对**单独价值函数**的需求，减少了训练时的**内存占用**和模型训练的**复杂度**，同时避免了**价值函数的训练误差**对优势估计的影响。

**解析**：GRPO通过**组内奖励标准化**替代PPO的价值函数，简化了模型架构，同时让优势估计更贴合推理任务的奖励分布。

### 10. 简答题（3+4分）
**题目**：测试时缩放（test-time scaling）。(1) 定义测试时缩放；(2) 列出课堂上讨论的两种在推理时控制或增加“思考”预算的方法。

**答案**：
1. 测试时缩放指**不增加模型参数或训练数据**，而是通过**在推理阶段增加计算量**的方式，提升模型的推理性能的方法。
2. 方法一：**Best-of-N**（生成多个解决方案并通过验证筛选最优）；方法二：**上下文感知/预算强制**（Context awareness, budget forcing）（为模型分配固定的推理令牌数，强制其完成分步思考）。

**解析**：**测试时缩放是推理阶段的优化策略**，核心是用“**计算量换性能**”，无需重新训练模型，是提升LLM推理能力的高效手段。

## Lecture 7：Agentic LLMs

### 1. 选择题（1分）
**题目**：检索增强生成（RAG）主要解决冻结LLM的哪个局限性？
A. 大参数量带来的固有延迟问题和慢推理速度
B. 模型无法执行复杂的浮点算术计算
**C. 知识截止（knowledge cutoff）和缺乏私有领域知识**
D. 缺乏内置的安全护栏，防止生成有毒或有害内容

**答案**：C

**解析**：冻结的LLM的知识仅来自**预训练语料**，存在**知识截止时间**（无法获取最新知识），且无企业/个人的**私有领域知识**；RAG通过**将LLM与外部知识库对接，推理时检索相关知识并融入生成，解决该问题**；A中RAG无法降低推理延迟，B中算术计算可通过工具解决，D中安全护栏是对齐技术的目标，均错误。

### 2. 选择题（1分）
**题目**：在RAG中，“上下文检索（contextual retrieval）”指的是？
A. 检索整个数据库内容并将其塞入上下文窗口
B. 在生成阶段忽略模型上下文窗口的硬限制
**C. 为文本块添加上下文（如文档摘要），以提高检索准确率**
D. 将检索机制限制为仅查看查询的最后一句话

**答案**：C

**解析**：**上下文检索**是RAG的检索优化方法，核心是**为知识库中的每个文本块（chunk）补充额外的上下文信息**（如文档标题、摘要、段落位置），让检索模型能更准确地理解文本块的含义，提升与查询的匹配度；A会导致上下文窗口溢出，B无法突破窗口限制，D会丢失查询的关键信息，均错误。

### 3. 选择题（1分）
**题目**：在智能体（agents）的语境中，ReAct框架是？

A. **“推理（Reasoning）”（规划）和“行动（Acting）”（执行、观察）的交错循环**
B. 一种流行的前端JavaScript库，用于构建交互式用户界面
C. 一种基于用户情绪反应重新训练模型的微调方法
D. 一种特定的稀疏注意力掩码，旨在优化Transformer的效率

**答案**：A

**解析**：ReAct是智能体的核心框架，将**推理和行动结合**：模型先**根据当前状态推理并制定行动规划**，再**执行行动并观察环境的反馈**，最后**根据反馈更新状态并进入下一轮推理**，形成闭环；BCD均与ReAct框架无关，错误。

### 4. 选择题（2分）
**题目**：在工具使用场景中，若LLM输出一个函数调用（如find teddy bear(loc=‘Stanford’)），通常由谁执行该函数调用？
A. LLM自身在内部模拟执行，并幻觉返回值
**B. 外部运行时/系统（后端），执行后将结果返回给LLM**
C. 人类用户手动执行代码，并将结果重新输入到聊天上下文中
D. 单独的奖励模型拦截调用并计算结果

**答案**：B

**解析**：LLM仅负责**生成工具调用指令**，不具备直接执行代码/函数的能力，工具调用的执行由**外部的后端系统/运行时**完成，执行后的结果会作为**观察值返回给LLM**，LLM再**根据该结果生成最终回复**；A中LLM不会模拟执行（易幻觉），C中无需人类手动执行，D中奖励模型无执行工具的功能，均错误。

### 5. 选择题（2分）
**题目**：模型上下文协议（MCP）旨在标准化？
A. 标准Transformer块的内部架构和层归一化
B. 预训练中使用的交叉熵损失函数的数学公式
C. 用于在标准基准上评分性能的特定评估指标
**D. LLM客户端/主机与数据源/工具（服务器）之间的连接**

**答案**：D

**解析**：**MCP是为Agentic LLMs设计的标准化协议**，核心是定义**LLM与外部数据源、工具、服务之间的通信接口**，让不同的LLM和外部工具能无缝对接，实现工具调用的标准化；ABC均非该协议的标准化目标，错误。

### 6. 选择题（1分）
**题目**：标准聊天机器人与“智能体（agent）”的关键区别是？
A. 智能体严格要求处理图像、音频等多模态输入
B. 聊天机器人在任何情况下都无法访问互联网
**C. 智能体通常具有自主性，能代表用户追求目标并执行多步行动**
D. 智能体依赖循环神经网络（RNN）而非Transformer架构

**答案**：C

**解析**：标准聊天机器人仅能根据**用户提示词**生成文本回复，无自主规划和行动能力；而智能体的核心特征是**自主性和目标导向，能理解用户的目标，自主制定多步行动规划，并执行工具调用等行动来实现目标**；A中智能体不强制多模态，B中聊天机器人可通过插件访问互联网，D中智能体多基于Transformer，均错误。

### 7. 选择题（2分）
**题目**：当智能体幻觉出不存在的工具时，常见的补救措施是？
**A. 在系统提示词中更新清晰的工具定义，或检查工具路由器的逻辑错误**
B. 将采样温度提高到1.0，鼓励生成更具创造性的输出
C. 完全禁用所有工具功能，恢复为纯文本生成模式
D. 切换到参数量显著更小的模型，以降低输出的复杂度

**答案**：A

**解析**：智能体幻觉不存在的工具，核心原因是**对工具的定义/可用范围不明确**，或**工具路由器**（决定是否调用工具、调用哪个工具）的逻辑存在错误；因此最直接的补救措施是**在系统提示词中明确工具的名称、参数、使用场景，同时修复路由器的逻辑**；B中提高温度会加剧幻觉，C是极端做法（丧失工具能力），D中小模型的推理和工具调用能力更弱，均错误。

### 8. 选择题（2分）
**题目**：A2A（Agent2Agent）协议旨在促进？
A. 人类用户与AI智能体之间的标准通信接口
**B. 不同专业智能体之间的通信和任务交接**
C. 两种不同基础模型架构之间的迁移学习技术
D. 将自主智能体对齐到严格的人类道德价值观的过程

**答案**：B

**解析**：A2A协议是**智能体间的通信标准**，核心是实现**不同的专业智能体之间的无缝通信和任务交接**（如一个数学推理智能体将复杂的编程任务交接给代码智能体），构建多智能体协作系统；A是人类与智能体的通信，C是迁移学习，D是智能体对齐，均非该协议的目标，错误。

### 9. 简答题（3+3分）
**题目**：RAG与长上下文。(1) 为何即使模型有100万令牌的上下文窗口，仍可能使用RAG？(2) 说出一个RAG系统特有的挑战。
**答案**：
1. 第一、RAG的**成本更低**（输入令牌更少，推理费用低）、**速度更快**（延迟更低）；第二、即使是长上下文模型，仍会面临“**大海捞针**”问题——无法从**海量的上下文信息**中有效检索到关键信息，而RAG通过**精准的检索**能直接定位相关知识。
2. **检索准确率问题**：模型可能检索到与查询无关的文本块，或遗漏关键的相关文本块（RAG特有的挑战还可答：文本块分割的合理性、知识库的更新与维护、检索结果的融合质量等）。
   
**解析**：长上下文模型虽能容纳大量信息，但在信息检索的**效率、准确性和成本**上仍不如RAG，因此**RAG仍是长文本处理的主流方案**。

### 10. 简答题（3+4分）
**题目**：工具调用工作流。(1) 从LLM的角度，描述工具执行循环的三个高层步骤；(2) 若工具执行返回“错误”，会发生什么？

**答案**：
1. 首先、LLM根据用户的**查询和当前状态**，**生成工具调用指令**（判断是否需要调用工具、调用哪个工具、传入什么参数）；然后、**外部后端**执行工具调用，LLM**等待工具的执行结果/观察值**；最后、LLM将工具执行结果**融入上下文**，**生成最终的自然语言回复**（若结果不完整，可能触发新一轮工具调用）。
2. 工具的**错误信息**会作为**观察值**返回给LLM；一个能力较强的智能体会**分析错误的原因**（如参数错误、工具不可用、查询条件无效），并尝试**纠正参数、更换工具或调整查询策略**，重新发起工具调用。

**解析**：**工具调用**是智能体的核心能力，其执行循环是一个闭环，错误反馈会被智能体利用做后续的策略调整，而非直接终止生成。

## Lecture 8：LLM evaluation

### 1. 选择题（1分）
**题目**：在LLM基准测试的语境中，古德哈特定律（Goodhart’s Law）表明？
A. 所有标准化基准测试对于衡量模型性能本质上是无用的
B. 大参数量模型在所有任务上都会普遍优于小参数量模型
C. 人类评估始终是完全一致的，且无主观偏差
**D. 当一个基准测试成为优化目标时，它就不再是一个好的衡量标准**

**答案**：D

**解析**：古德哈特定律的核心是“**当一个指标成为目标时，它就不再是一个有效的指标**”；在LLM中，若模型针对某个基准测试做**专门的优化**（如过拟合、记忆测试集），则该基准测试将**无法再真实反映模型的通用性能**；A中基准测试并非完全无用，B中模型性能并非仅由参数量决定，C中人类评估存在显著的主观偏差，均错误。

### 2. 选择题（1分）
**题目**：评估中的“数据污染（data contamination）”指的是？
A. 监督微调数据集中包含有毒或有害内容
B. 将SFT数据格式化为模型无法解析的JSON结构
C. 在推理时向提示词嵌入中注入随机高斯噪声
**D. 评估/测试数据泄露到模型的预训练语料中**

**答案**：D

**解析**：LLM评估中的数据污染是指**测试集/评估基准的样本被包含在模型的预训练语料中**，模型在评估时并非通过**推理能力**解决问题，而是通过**记忆样本**得到高分，导致评估结果失真；ABC均非数据污染的定义，错误。

### 3. 选择题（1分）
**题目**：“LLM-as-a-judge”通常指的是？
**A. 使用强模型（如Gemini 3 Pro），根据评分标准为其他模型的输出打分**
B. 使用一个非常小的蒸馏模型，评估一个大得多的模型的能力
C. 让模型输出对自身生成内容的置信度分数
D. 利用一个专门为二分类情感分析微调的传统BERT模型

**答案**：A

**解析**：LLM-as-a-judge是一种**自动化评估方法**，核心是使用**性能更强的大模型**作为“裁判”，为待评估模型的输出根据预设的**评分标准**（如有用性、准确性、流畅性）打分，**替代部分人类评估**；B中小模型无足够的评估能力，C中模型的自置信度不可靠，D中BERT仅适用于特定分类任务，无法做通用的LLM输出评估，均错误。

### 4. 选择题（2分）
**题目**：以下哪项是“成对（pairwise）”评估方法？
A. 为单个回复分配一个绝对的标量质量分数
**B. 展示两个模型回复A和B，并询问“哪个更好？”**
C. 计算回复与参考文本之间的n-gram重叠BLEU分数
D. 运行静态分析工具，验证生成的代码是否能编译成功

**答案**：B

**解析**：成对评估是指**对比两个模型输出的优劣**，而非为单个输出打分，核心是获取**相对偏好**；A是单样本的绝对评分，C是基于n-gram的自动评估，D是可验证的客观评估，均非成对评估，错误。

### 5. 选择题（2分）
**题目**：LLM-as-a-judge中的“位置偏差（position bias）”指的是？
A. 模型表现出对符合特定政治意识形态的答案的偏好
B. 模型仅关注提示词末尾的指令
**C. 模型偏爱特定位置的选项，而与内容质量无关**
D. 模型系统性地为长得多的答案分配更高的分数

**答案**：C

**解析**：LLM-as-a-judge中的位置偏差是指作为裁判的LLM，会受到**回复展示位置**的影响（如始终偏爱第一个展示的回复），而忽略回复本身的质量，导致评估结果失真；A是意识形态偏差，B是提示词的位置偏差（非评估偏差），D是冗长性偏差（verbosity bias），均错误。

### 6. 选择题（2分）
**题目**：“Pass@K”指标的定义是？
**A. 生成的k个解决方案中，至少有一个正确的概率**
B. 生成的k个解决方案中，每一个都正确的概率
C. 第一次生成尝试严格通过的测试用例的百分比
D. 排行榜上前k个模型的平均质量分数

**答案**：A

**解析**：**Pass@K是代码生成、数学推理等任务的核心评估指标**，计算方式为：对每个测试用例，模型生成k个候选解决方案，**至少有一个解决方案正确**的测试用例占总测试用例的比例（概率）；B是全对的概率，C是pass@1的片面理解，D与该指标无关，均错误。

### 7. 选择题（2分）
**题目**：哪个基准测试主要用于测试跨学科（如STEM、人文、社会科学）的多任务知识？
**A. MMLU**
B. GSM8K
C. HumanEval
D. HarmBench

**答案**：A

**解析**：MMLU（Massive Multitask Language Understanding）是**多任务语言理解基准**，涵盖STEM、人文、社会科学、商业等57个学科的选择题，测试模型的跨学科知识和推理能力；GSM8K是**小学数学推理基准**，HumanEval是**代码生成基准**，HarmBench是LLM**安全性评估基准**，均错误。

### 8. 选择题（1分）
**题目**：为何BLEU、ROUGE等n-gram指标通常被认为不足以评估现代开放式聊天LLM？
A. 它们在大数据集上的计算成本过高、速度过慢
B. 它们严格需要高端GPU加速才能有效计算
C. 它们在语言上有局限性，本质上只能处理英文文本
**D. 它们关注精确的词重叠，而非语义含义和推理正确性**

**答案**：D

**解析**：BLEU、ROUGE等n-gram指标的核心是计算**生成文本与参考文本之间的词/短语重叠度**，而开放式聊天LLM的输出具有**多样性**，相同的语义可通过不同的表述实现，且这些指标无法衡量文本的语义**正确性、推理能力和连贯性**；A中其计算成本较低，B中无需GPU加速，C中可支持多语言，均错误。

### 9. 简答题（3+4分）
**题目**：LLM-as-a-judge偏差。(1) 定义“冗长性偏差（verbosity bias）”；(2) 提出一种具体方法，缓解你选择的任意一种偏差。

**答案**：
1. **冗长性偏差**指作为裁判的LLM，**倾向于偏爱更长的回复**，即使更长的回复存在**内容重复、逻辑冗余或准确性更低**的问题，而忽略简洁、准确的短回复。

2. 以**位置偏差**为例，缓解方法为**位置交换（position swapping）**：将成对的回复（A,B）先按A在前、B在后的顺序让LLM评估，再按B在前、A在后的顺序重新评估，若两次评估结果**一致则保留，不一致则剔除或重新评估**，消除位置对评估结果的影响。
3. 
**解析**：LLM-as-a-judge存在多种**固有偏差**，需通过**针对性**的方法消除，确保评估结果的**客观性**；也可选择缓解冗长性偏差，方法如在评分标准中明确“**简洁性**”为评估维度，或对回复长度做**归一化处理**。

### 10. 简答题（3+3分）
**题目**：基准测试。(1) SWE-bench基准测试评估什么能力？(2) 解释静态基准测试（如MMLU）与动态排行榜（如Chatbot Arena）的区别。

**答案**：
1. SWE-bench是**软件工程基准测试**，评估模型解决**真实世界软件工程问题**的能力，具体为模型能否修复流行Python仓库中的GitHub Issues（如代码bug修复、功能实现、文档完善）。
2. **静态基准测试**使用**固定的测试用例/答案集**，评估指标明确，但易出现**数据污染**（测试集泄露到预训练语料）；**动态排行榜**依赖**实时的人类投票**（如Chatbot Arena的匿名成对比较），评估结果更贴合人类的实际使用体验，不易被数据污染影响，但评估结果**存在主观偏差，且指标相对模糊**。

**解析**：静态基准测试和动态排行榜各有优劣，前者适合模型的**定量、可复现评估**，后者适合模型的**定性、实际体验评估**，通常结合使用以**全面衡量LLM性能**。